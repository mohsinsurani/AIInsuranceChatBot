# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AIrDIaF85i5hGd4t3LjUw73qSPCtHyUI
"""

!pip install TensorFlow --quiet
!pip install gradio --quiet
!pip install xformer --quiet
!pip install chromadb --quiet
!pip install langchain --quiet
!pip install accelerate --quiet
!pip install transformers --quiet
!pip install bitsandbytes --quiet
!pip install unstructured --quiet
!pip install sentence-transformers==2.2.2 --quiet
!pip install PyPDF2 --quiet
!pip install streamlit --quiet
!pip install InstructorEmbedding==1.0.1
!pip install faiss-gpu





import streamlit as st
from PyPDF2 import PdfReader, PdfFileReader
from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)
from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
from langchain.schema.runnable import RunnablePassthrough

from PyPDF2 import PdfReader
from langchain.text_splitter import (
    CharacterTextSplitter,
)
from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings
from langchain_community.vectorstores import FAISS
import pickle
import os.path


class EmbeddingsPDF:
    def get_pdf_text(self, pdf_docs):
        text = ""
        for pdf in pdf_docs:
            pdf_reader = PdfReader(pdf)
            for page in pdf_reader.pages:
                if page is not None:
                    page_text = page.extract_text()
                    if page_text is not None:
                        text += page_text
        return text

    def get_text_chunks(self, raw_text):
        text_splitter = CharacterTextSplitter(
            separator="\n", chunk_size=512, chunk_overlap=200, length_function=len
        )
        chunks = text_splitter.split_text(raw_text)
        return chunks

    def get_vector_store(self, text_chunks):
        file_path = "embed.pickle"
        if os.path.isfile(file_path):
            vectorstore = pickle.load(file_path)
            return vectorstore
        else:
            embeddings = HuggingFaceInstructEmbeddings(
                model_name="hkunlp/instructor-xl"
            )
            vectorstore = FAISS.from_texts(text_chunks, embeddings)
            with open(file_path, "wb") as file:
                pickle.dump(vectorstore, file)
            return vectorstore

    def get_embeddings(self):
        file_path = "/content/embed.pickle"
        if os.path.isfile(file_path):
            with open(file_path, "rb") as file:
                vectorstore = pickle.load(file)
                return vectorstore
        else:
            # 1 Get the text of the pdf
            pdf_docs = open("/content/healthInsurance.pdf", "rb")
            raw_text = self.get_pdf_text([pdf_docs])
            # 2 Get the text chunks
            text_chunks = self.get_text_chunks(raw_text)

            # 3 Get the vector store
            text_vector_store = self.get_vector_store(text_chunks=text_chunks)
            return text_vector_store

vectorstore = EmbeddingsPDF().get_embeddings()

from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

MODEL_NAME = "susnato/phi-2"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, return_token_type_ids=False)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    trust_remote_code=True,
    device_map="auto",
    quantization_config=None  # Remove quantization for now, add it back later if needed
)

generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
generation_config.max_new_tokens = 1024
generation_config.temperature = 0.0001
generation_config.top_p = 0.95
generation_config.do_sample = True
generation_config.repetition_penalty = 1.15

from transformers import TextGenerationPipeline

text_generation_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, generation_config=generation_config)
from langchain import HuggingFacePipeline

llm = HuggingFacePipeline(
    pipeline=text_generation_pipeline,
)

from langchain import PromptTemplate
from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain

template = """
[INST] <>
Act as an Insurance expert. Use the following information to answer the question at the end.
<>

{context}

{question} [/INST]
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
    )


# qa_chain.llm_model.config.max_length = 512  # Set max sequence length

from IPython.display import Markdown, display
from tqdm import tqdm

query = "What is Vitality?"

with tqdm(total=1, desc="Processing queries") as pbar:
    with torch.autograd.profiler.profile(use_cuda=True) as prof:
        result_ = qa_chain(query)
        result = result_["result"].strip()
        print(result)
    pbar.update(1)


display(Markdown(f"<b>{query}</b>"))
display(Markdown(f"<p>{result}</p>"))

import gc
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024"
def clear_gpu_memory():
    torch.cuda.empty_cache()
    gc.collect()

clear_gpu_memory()

pdf_document.page_count

loaders = [UnstructuredPDFLoader(filename)]

!apt-get install poppler-utils # error occurs without this, pdf rendering library

pip install tensorflow-probability==0.13.0

pip install tiktoken

index = VectorstoreIndexCreator().from_loaders(loaders)

if pdf_file is not None:
    # Example: Read PDF text using PyMuPDF
    import fitz
    pdf_document = fitz.open(pdf_document)
    pdf_text = ""
    for page_num in range(pdf_document.page_count):
        pdf_text += pdf_document[page_num].get_text()

    input_text = pdf_text  # You can modify this based on your requirements

prompt = st.text_area("Enter your prompt", """Cosmetic Treatment""")

prompt

import torch

# Assuming tokenizer and model are already loaded

# Maximum sequence length supported by the model
max_sequence_length = 1022

# Step 4: Get the prompt from the user
prompt = input("Enter your prompt: ")

# Step 5: Combine PDF text with the prompt
input_text = prompt + " " + pdf_text

# Step 6: Feed the data into the model and get the output
with torch.no_grad():
    # Tokenize the input text
    token_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors="pt")

    # Check if the input exceeds the maximum sequence length
    if token_ids.size(1) > max_sequence_length:
        # Truncate or split the input into smaller chunks
        input_chunks = [token_ids[:, i:i+max_sequence_length-2] for i in range(0, token_ids.size(1), max_sequence_length-2)]
        outputs = []

        for chunk in input_chunks:
            output_ids = model.generate(
                chunk.to(model.device),
                max_length=max_sequence_length,  # Adjust the maximum length if needed
                do_sample=True,
                temperature=0.3,
                pad_token_id=tokenizer.eos_token_id  # Use eos_token_id as pad_token_id for open-end generation
            )
            outputs.append(output_ids)

        # Concatenate the outputs and get the last one
        output_ids = torch.cat(outputs, dim=1)
        output_ids = output_ids[:, -max_sequence_length+2:]
    else:
        # Input fits within the maximum sequence length
        output_ids = model.generate(
            token_ids.to(model.device),
            max_length=max_sequence_length,  # Adjust the maximum length if needed
            do_sample=True,
            temperature=0.3,
            pad_token_id=tokenizer.eos_token_id  # Use eos_token_id as pad_token_id for open-end generation
        )

# Decode the generated output
output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print("Generated Output:")
print(output)

st.text("Generated Output:")
st.write(output)

output.encode('utf-8')